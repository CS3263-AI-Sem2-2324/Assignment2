{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS3263 - Assignment 2\n",
    "---\n",
    "#### Sem 2, AY 2023/24\n",
    "\n",
    "👋 Welcome to the Assignment 2 of course CS3263 - Foundations of Artificial Intelligence! \n",
    "\n",
    "# Guideline\n",
    "\n",
    "- Please complete this Assignment in a **TWO-person team**.\n",
    "- **ONE person from each team** will need to submit **ALL** the answers.\n",
    "- Grading will be done team-wise. Please register your groups on Canvas.\n",
    "- The deadline for the assignment is **`12 April 2024, 2359`**.\n",
    "\n",
    "You are encouraged to discuss solution ideas. However, each team *must write up the solutions independently*. It is considered plagiarism if the solution write-up is highly similar to other teams' write-ups or other sources.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "# Team Info\n",
    "\n",
    "- **Group Member 1:**\n",
    "    * Name:\n",
    "    * Student ID:\n",
    "\n",
    "- **Group Member 2:**\n",
    "    * Name:\n",
    "    * Student ID:\n",
    "\n",
    "---\n",
    "\n",
    "# Submission\n",
    "\n",
    "- You can download the programming package from Canvas or from https://github.com/CS3263-AI-Sem2-2324/Assignment2.\n",
    "- No late submissions are allowed.\n",
    "- Complete the codes in `main.ipynb` and **copy** your solutions to `programming.py`.\n",
    "- Please write the following on the top of your solution files:\n",
    "    - The **name(s)** and **metric number(s)** of ALL team members as they appear on Canvas.\n",
    "    - Collaborators (write *None* if no collaborators).\n",
    "    - Sources, if any, or if you obtained the solutions through research, e.g., through the web.\n",
    "- Your programming part should be zipped and named `netid1-netid2.zip`. **Only submit the zipped file to Canvas**.\n",
    "- **IMPORTANT:** The programming part will be auto-graded on the [CodePost](https://codepost.io/) platform.\n",
    "    - You don't need to submit the code to CodePost. We will import your submissions from Canvas.\n",
    "    - Note that the `programming.py` file is designed for auto-grading, so you cannot run it normally from your end but don't worry.\n",
    "    - DO NOT modify any code outside the blocks you're allowed to; DO NOT print any message in `programming.py`.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming Assignment\n",
    "In this programming assignment, we are going to solve the searching problems in a gird maze environment.\n",
    "\n",
    "This notebook will walk you through how to use the environment, how to define your own agent, and how to solve the problems.\n",
    "\n",
    "You are expected to add your codes in the blocks noted by:\n",
    "```python\n",
    "# ------- your code starts here ----- #\n",
    "\n",
    "# Some code examples ...\n",
    "\n",
    "# ------- your code ends here ------- #\n",
    "```\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Environment Setup\n",
    "To set up the Python programming environment, three basic packages, i.e., `typer`, `typing`, and `numpy` are needed.\n",
    "\n",
    "You can install them by the way you like.\n",
    "\n",
    "- For example, install using `pip`:\n",
    "\n",
    "    ```\n",
    "    pip install typer numpy\n",
    "    ```\n",
    "\n",
    "- Or install using `conda`:\n",
    "\n",
    "    ```\n",
    "    conda install typer numpy\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from typing import Callable\n",
    "from minihouse.robotminihousemodel import MiniHouseV1\n",
    "from minihouse.minihousemodel import state_class, item_object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minihouse Environment\n",
    "All environments are defined in the `minihouse` package.\n",
    "\n",
    "Each of the **states** and **actions** are represented as an integer.\n",
    "\n",
    "You can use the following functions to convert between the integer and the corresponding state description.\n",
    "\n",
    "```python\n",
    "goal_state = state_class(\n",
    "    robot_agent=None,\n",
    "    human_agent=None,\n",
    "    object_list={\"apple\": item_object(\"apple\", True, \"table\")},\n",
    "    container_list=None,\n",
    "    surface_list=None,\n",
    "    room_list=None,\n",
    ")\n",
    "\n",
    "env = MiniHouseV1(\n",
    "    instruction=\"move the apple to the table\",\n",
    "    goal_state=goal_state,\n",
    ")\n",
    "env.reset()\n",
    "print(env.state)\n",
    "print(env.state_to_index(env.state))\n",
    "```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1: Policy Iteration & Value Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem, you are expected to implement the **policy iteration** and **value iteration** algorithms to solve the searching problems in the `MiniHouse` environment. \n",
    "\n",
    "Here, we provide a template for you to implement your own agents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mdp_solver:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.iteration = 0\n",
    "        print(\"MDP initialized!\")\n",
    "        \n",
    "    \n",
    "    def get_action_value(\n",
    "        self, s:int, a:int, V:np.ndarray, gamma:float, env_transition:Callable):\n",
    "        \"\"\"\n",
    "        Code for getting action value. Compute the value of taking action a in state s\n",
    "        I.e., compute Q(s, a) = \\sum_{s'} p(s'| s, a) * [r + gamma * V(s')]\n",
    "        args:\n",
    "            s: state\n",
    "            a: action\n",
    "            V: value function\n",
    "            gamma: discount factor\n",
    "            env_transition: transition function\n",
    "        returns:\n",
    "            value: action value\n",
    "        \"\"\"\n",
    "        value = 0\n",
    "        \n",
    "        for prob, next_state, reward, done in env_transition(s, a):\n",
    "            \n",
    "            # ------- your code starts here ----- #\n",
    "            \n",
    "            \n",
    "\n",
    "            # ------- your code ends here ------- #\n",
    "        \n",
    "        return value\n",
    "    \n",
    "    \n",
    "    def get_max_action_value(\n",
    "        self, s:int, env_nA:int, env_transition:Callable, V:np.ndarray, gamma:float):\n",
    "        \"\"\"\n",
    "        Code for getting max action value. Takes in the current state and returns \n",
    "        the max action value and action that leads to it. I.e., compute\n",
    "        a* = argmax_a \\sum_{s'} p(s'| s, a) * [r + gamma * V(s')]\n",
    "        args:\n",
    "            s: state\n",
    "            env_nA: number of actions\n",
    "            env_transition: transition function\n",
    "            V: value function\n",
    "            gamma: discount factor\n",
    "        returns:\n",
    "            max_value: max action value\n",
    "            max_action: action that leads to max action value\n",
    "        \"\"\"\n",
    "        max_value = -np.inf\n",
    "        max_action = -1\n",
    "        \n",
    "        for a in range(env_nA):\n",
    "            \n",
    "            # ------- your code starts here ----- #\n",
    "            \n",
    "            \n",
    "\n",
    "            # ------- your code ends here ------- #\n",
    "        \n",
    "        return max_value, max_action\n",
    "    \n",
    "    \n",
    "    def get_policy(\n",
    "        self, env_nS:int, env_nA:int, env_transition:Callable, gamma:float, V:np.ndarray):\n",
    "        \"\"\"\n",
    "        Code for getting policy. Takes in an Value function and returns the optimal policy\n",
    "        args:\n",
    "            env_nS: number of states\n",
    "            env_nA: number of actions\n",
    "            env_transition: transition function\n",
    "            gamma: discount factor\n",
    "            V: value function\n",
    "        returns:\n",
    "            policy: policy\n",
    "        \"\"\"\n",
    "        policy = np.zeros(env_nS)\n",
    "        \n",
    "        for s in range(env_nS):\n",
    "            max_value = -np.inf\n",
    "            max_action = -1\n",
    "            for a in range(env_nA):\n",
    "                \n",
    "                # ------- your code starts here ----- #\n",
    "                \n",
    "                \n",
    "                \n",
    "                # ------- your code ends here ------- #\n",
    "                    \n",
    "            policy[s] = max_action\n",
    "        \n",
    "        return policy\n",
    "    \n",
    "    \n",
    "    def policy_evaluation(\n",
    "        self, env_nS:int, env_transition:Callable, V:np.ndarray, gamma:float, theta:float, policy:np.ndarray):\n",
    "        \"\"\"\n",
    "        Code for policy evaluation. Takes in an MDP and returns the converged value function\n",
    "        args:\n",
    "            env_nS: number of states\n",
    "            env_transition: transition function\n",
    "            V: value function\n",
    "            gamma: discount factor\n",
    "            theta: convergence threshold\n",
    "            policy: policy\n",
    "        returns:\n",
    "            V: value function\n",
    "        \"\"\" \n",
    "        \n",
    "        while True:\n",
    "            delta = 0\n",
    "            for s in range(env_nS):\n",
    "                \n",
    "                # ------- your code starts here ----- #\n",
    "                \n",
    "                \n",
    "                \n",
    "                # ------- your code ends here ------- #\n",
    "                \n",
    "            if delta < theta:\n",
    "                break\n",
    "                \n",
    "        return V\n",
    "    \n",
    "    \n",
    "    def policy_improvement(\n",
    "        self, env_nS:int, env_nA:int, env_transition:Callable, policy:np.ndarray, V:np.ndarray, gamma:float):\n",
    "        \"\"\"\n",
    "        Code for policy improvement. Takes in an MDP and returns the converged policy\n",
    "        args:\n",
    "            env_nS: number of states\n",
    "            env_nA: number of actions\n",
    "            env_transition: transition function\n",
    "            policy: policy\n",
    "            V: value function\n",
    "            gamma: discount factor\n",
    "        returns:\n",
    "            policy_stable: whether policy is stable\n",
    "            policy: policy\n",
    "        \"\"\"\n",
    "        policy_stable = True\n",
    "        \n",
    "        for s in range(env_nS):\n",
    "            \n",
    "            # ------- your code starts here ----- #\n",
    "            \n",
    "            \n",
    "\n",
    "            # ------- your code ends here ------- #\n",
    "        \n",
    "        return policy_stable, policy\n",
    "    \n",
    "    \n",
    "    def value_iteration(\n",
    "        self, gamma:float, theta:float, env_nS:int, env_nA:int, env_transition:Callable):\n",
    "        \"\"\"\n",
    "        The code for value iteration. Takes in an MDP and returns the optimal policy\n",
    "        and value function.\n",
    "        args:\n",
    "            gamma: discount factor\n",
    "            theta: convergence threshold\n",
    "            env_nS: number of states\n",
    "            env_nA: number of actions\n",
    "            env_transition: transition function\n",
    "        returns:\n",
    "            policy: optimal policy\n",
    "            V: optimal value function \n",
    "        \"\"\"\n",
    "        V = np.zeros(env_nS)\n",
    "        converged = False\n",
    "        \n",
    "        while not converged:\n",
    "            delta = 0\n",
    "            \n",
    "            # ------- your code starts here ----- #\n",
    "            \n",
    "            \n",
    "\n",
    "            # ------- your code ends here ------- #\n",
    "        \n",
    "        policy = self.get_policy(env_nS, env_nA, env_transition, gamma, V)\n",
    "        return policy, V\n",
    "    \n",
    "    \n",
    "    def policy_iteration(\n",
    "        self, gamma:float, theta:float, env_nS:int, env_nA:int, env_transition:Callable):\n",
    "        \"\"\"\n",
    "        Code for policy iteration. Takes in an MDP and returns the optimal policy\n",
    "        args:\n",
    "            gamma: discount factor\n",
    "            theta: convergence threshold\n",
    "            env_nS: number of states\n",
    "            env_nA: number of actions\n",
    "            env_transition: transition function\n",
    "        returns:\n",
    "            policy: optimal policy\n",
    "            V: optimal value function\n",
    "        \"\"\"\n",
    "        V = np.zeros(env_nS)\n",
    "        policy = np.zeros(env_nS)\n",
    "        converged = False\n",
    "        \n",
    "        while not converged:\n",
    "            \n",
    "            # ------- your code starts here ----- #\n",
    "            \n",
    "            \n",
    "        \n",
    "            # ------- your code ends here ------- #\n",
    "        \n",
    "        return policy, V\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🔍 Guideline\n",
    "\n",
    "Here, we introduce the role of each function in the above solution template. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Action Value\n",
    "\n",
    "The function is as follows:\n",
    "\n",
    "```python\n",
    "    def get_action_value(\n",
    "        self, s:int, a:int, V:np.ndarray, gamma:float, env_transition:Callable):\n",
    "        \"\"\"\n",
    "        Code for getting action value. Compute the value of taking action a in state s\n",
    "        I.e., compute Q(s, a) = \\sum_{s'} p(s'| s, a) * [r + gamma * V(s')]\n",
    "        args:\n",
    "            s: state\n",
    "            a: action\n",
    "            V: value function\n",
    "            gamma: discount factor\n",
    "            env_transition: transition function\n",
    "        returns:\n",
    "            value: action value\n",
    "        \"\"\"\n",
    "        value = 0\n",
    "        # ------- your code starts here ----- #\n",
    "\n",
    "\n",
    "        # ------- your code ends here ------- #\n",
    "        return value\n",
    "```\n",
    "This function is used to get the action value. The input arguments are:\n",
    "* `s`: state\n",
    "* `a`: action\n",
    "* `V`: value function, which is a list of float numbers, each float number represents the value for the corresponding state index.\n",
    "* `gamma`: discount factor\n",
    "* `env_transition`: transition function, which returns a list of tuples (probability, next_state_index, reward, done), where probability is a float number, next_state_index is an integer, reward is a float number, done is a boolean value indicating whether the task is finished.\n",
    "\n",
    "The output arguments are:\n",
    "* `value`: action value, which is a float number.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Max Action Value\n",
    "\n",
    "The function is as follows:\n",
    "\n",
    "```python\n",
    "    def get_max_action_value(\n",
    "        self, s:int, env_nA:int, env_transition:Callable, V:np.ndarray, gamma:float):\n",
    "        \"\"\"\n",
    "        Code for getting max action value. Takes in the current state and returns \n",
    "        the max action value and action that leads to it. I.e., compute\n",
    "        a* = argmax_a \\sum_{s'} p(s'| s, a) * [r + gamma * V(s')]\n",
    "        args:\n",
    "            s: state\n",
    "            env_nA: number of actions\n",
    "            env_transition: transition function\n",
    "            V: value function\n",
    "            gamma: discount factor\n",
    "        returns:\n",
    "            max_value: max action value\n",
    "            max_action: action that leads to max action value\n",
    "        \"\"\"\n",
    "        max_value = -np.inf\n",
    "        max_action = -1\n",
    "        # ------- your code starts here ----- #\n",
    "\n",
    "\n",
    "        # ------- your code ends here ------- #\n",
    "        return max_value, max_action\n",
    "\n",
    "```\n",
    "This function is used to get the max action value. The input arguments are:\n",
    "* `s`: state\n",
    "* `env_nA`: number of actions\n",
    "* `env_transition`: transition function, which returns a list of tuples (probability, next_state_index, reward, done), where probability is a float number, next_state_index is an integer, reward is a float number, done is a boolean value indicating whether the task is finished.\n",
    "* `V`: value function, which is a list of float numbers, each float number represents the value for the corresponding state index.\n",
    "* `gamma`: discount factor\n",
    "\n",
    "The output arguments are:\n",
    "* `max_value`: max action value, which is a float number.\n",
    "* `max_action`: action that leads to max action value, which is an integer.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Policy\n",
    "\n",
    "The function is as follows:\n",
    "\n",
    "```python\n",
    "    def get_policy(\n",
    "        self, env_nS:int, env_nA:int, env_transition:Callable, gamma:float, V:np.ndarray):\n",
    "        \"\"\"\n",
    "        Code for extracting a policy given a value function\n",
    "        args:\n",
    "            env_nS: number of states\n",
    "            env_nA: number of actions\n",
    "            env_transition: transition function\n",
    "            gamma: discount factor\n",
    "            V: value function\n",
    "        returns:\n",
    "            policy: optimal policy\n",
    "        \"\"\"\n",
    "        policy = np.zeros(env_nS, dtype=int)\n",
    "        # ------- your code starts here ----- #\n",
    "\n",
    "\n",
    "        # ------- your code ends here ------- #\n",
    "        return policy\n",
    "\n",
    "```\n",
    "This function is used to extract a policy given a value function. The input arguments are:\n",
    "* `env_nS`: number of states\n",
    "* `env_nA`: number of actions\n",
    "* `env_transition`: transition function, which returns a list of tuples (probability, next_state_index, reward, done), where probability is a float number, next_state_index is an integer, reward is a float number, done is a boolean value indicating whether the task is finished.\n",
    "* `gamma`: discount factor\n",
    "* `V`: value function, which is a list of float numbers, each float number represents the value for the corresponding state index.\n",
    "\n",
    "The output arguments are:\n",
    "* `policy`: optimal policy, which is a list of integers, each integer represents the action index for the corresponding state index.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Evaluation\n",
    "\n",
    "The function is as follows:\n",
    "\n",
    "```python\n",
    "    def policy_evaluation(\n",
    "        self, env_nS:int, env_transition:Callable, V:np.ndarray, gamma:float, theta:float, policy:np.ndarray):\n",
    "        \"\"\"\n",
    "        Code for policy evaluation. Takes in an MDP and returns the converged value function\n",
    "        args:\n",
    "            env_nS: number of states\n",
    "            env_transition: transition function\n",
    "            V: value function\n",
    "            gamma: discount factor\n",
    "            theta: convergence threshold\n",
    "            policy: policy\n",
    "        returns:\n",
    "            V: value function\n",
    "        \"\"\" \n",
    "        # ------- your code starts here ----- #\n",
    "\n",
    "\n",
    "        # ------- your code ends here ------- #\n",
    "        return V\n",
    "```\n",
    "This function is used to implement policy iteration. You are expected to implement the policy evaluation algorithm in this function. The input arguments are:\n",
    "* `env_nS`: number of states\n",
    "* `env_transition`: transition function, which returns a list of tuples (probability, next_state_index, reward, done), where probability is a float number, next_state_index is an integer, reward is a float number, done is a boolean value indicating whether the task is finished.\n",
    "* `V`: value function, which is a list of float numbers, each float number represents the value for the corresponding state index.\n",
    "* `gamma`: discount factor\n",
    "* `theta`: convergence threshold\n",
    "* `policy`: policy, which is a list of integers, each integer represents the action index for the corresponding state index.\n",
    "\n",
    "The output arguments are:\n",
    "* `V`: value function, which is a list of float numbers, each float number represents the value for the corresponding state index.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Improvement\n",
    "\n",
    "The function is as follows:\n",
    "\n",
    "```python\n",
    "    def policy_improvement(\n",
    "        self, env_nS:int, env_nA:int, env_transition:Callable, policy:np.ndarray, V:np.ndarray, gamma:float):\n",
    "        \"\"\"\n",
    "        Code for policy improvement. Takes in an MDP and returns the converged policy\n",
    "        args:\n",
    "            env_nS: number of states\n",
    "            env_nA: number of actions\n",
    "            env_transition: transition function\n",
    "            policy: policy\n",
    "            V: value function\n",
    "            gamma: discount factor\n",
    "        returns:\n",
    "            policy_stable: whether policy is stable\n",
    "            policy: policy\n",
    "        \"\"\"\n",
    "        policy_stable = True\n",
    "        # ------- your code starts here ----- #\n",
    "\n",
    "\n",
    "        # ------- your code ends here ------- #\n",
    "        return policy_stable, policy\n",
    "```\n",
    "This function is used to implement policy iteration. You are expected to implement the policy improvement algorithm in this function. The input arguments are:\n",
    "* `env_nS`: number of states\n",
    "* `env_nA`: number of actions\n",
    "* `env_transition`: transition function, which returns a list of tuples (probability, next_state_index, reward, done), where probability is a float number, next_state_index is an integer, reward is a float number, done is a boolean value indicating whether the task is finished.\n",
    "* `policy`: policy, which is a list of integers, each integer represents the action index for the corresponding state index.\n",
    "* `gamma`: discount factor\n",
    "* `V`: value function, which is a list of float numbers, each float number represents the value for the corresponding state index.\n",
    "\n",
    "\n",
    "The output arguments are:\n",
    "* `olicy_stable`: whether policy is stable, which is a boolean value.\n",
    "* `policy`: optimal policy, which is a list of integers, each integer represents the action index for the corresponding state index.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iteration\n",
    "\n",
    "The function is as follows:\n",
    "\n",
    "```python\n",
    "    def value_iteration(\n",
    "        self, gamma:float, theta:float, env_nS:int, env_nA:int, env_transition:Callable):\n",
    "        \"\"\"\n",
    "        The code for value iteration. Takes in an MDP and returns the optimal policy\n",
    "        and value function.\n",
    "        args:\n",
    "            gamma: discount factor\n",
    "            theta: convergence threshold\n",
    "            env_nS: number of states\n",
    "            env_nA: number of actions\n",
    "            env_transition: transition function\n",
    "        returns:\n",
    "            policy: optimal policy\n",
    "            V: optimal value function \n",
    "        \"\"\"\n",
    "        V = np.zeros(env_nS)\n",
    "        converged = False\n",
    "        # ------- your code starts here ----- #\n",
    "\n",
    "\n",
    "        # ------- your code ends here ------- #\n",
    "        policy = self.get_policy(env_nS, env_nA, env_transition, gamma, V)\n",
    "        return policy, V\n",
    "```\n",
    "This function is used to implement value iteration. You are expected to implement the value iteration algorithm in this function. The input arguments are:\n",
    "* `gamma`: discount factor\n",
    "* `theta`: convergence threshold\n",
    "* `env_nS`: number of states\n",
    "* `env_nA`: number of actions\n",
    "* `env_transition`: transition function, which returns a list of tuples (probability, next_state_index, reward, done), where probability is a float number, next_state_index is an integer, reward is a float number, done is a boolean value indicating whether the task is finished.\n",
    "\n",
    "The output arguments are:\n",
    "* `policy`: optimal policy, which is a list of integers, each integer represents the action index for the corresponding state index.\n",
    "* `V`: optimal value function, which is a list of float numbers, each float number represents the value for the corresponding state index.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Iteration\n",
    "\n",
    "The function is as follows:\n",
    "\n",
    "```python\n",
    "    def policy_iteration(\n",
    "        self, gamma:float, theta:float, env_nS:int, env_nA:int, env_transition:Callable):\n",
    "        \"\"\"\n",
    "        Code for policy iteration. Takes in an MDP and returns the optimal policy\n",
    "        and value function.\n",
    "        args:\n",
    "            gamma: discount factor\n",
    "            theta: convergence threshold\n",
    "            env_nS: number of states\n",
    "            env_nA: number of actions\n",
    "            env_transition: transition function\n",
    "        returns:\n",
    "            policy: optimal policy\n",
    "            V: optimal value function\n",
    "        \"\"\"\n",
    "        V = np.zeros(env_nS)\n",
    "        policy = np.zeros(env_nS)\n",
    "        converged = False\n",
    "        # ------- your code starts here ----- #\n",
    "\n",
    "\n",
    "        # ------- your code ends here ------- #\n",
    "        \n",
    "        return policy, V\n",
    "    \n",
    " ```\n",
    " \n",
    "This function is utilized to conduct policy iteration. Through policy iteration, you iteratively improve the policy until it converges to the optimal policy. The process involves two main steps: policy evaluation, where you calculate the value function for a given policy, and policy improvement, where you generate a new policy based on the calculated value function.\n",
    "\n",
    "The input arguments for this function are:\n",
    "\n",
    "- `gamma`: The discount factor, a float that determines the importance of future rewards.\n",
    "- `theta`: The convergence threshold, a small float value that determines when to stop iterating because the value function has sufficiently converged.\n",
    "- `env_nS`: The number of states in the environment, an integer.\n",
    "- `env_nA`: The number of actions available in the environment, an integer.\n",
    "- `env_transition`: The transition function of the environment. It takes a state and an action as input and returns a list of tuples (probability, next_state_index, reward, done). Each tuple represents a possible outcome of taking the action in the given state, where:\n",
    "    - `probability` is a float representing the likelihood of the outcome.\n",
    "    - `next_state_index` is an integer index of the next state.\n",
    "    - `reward` is a float representing the received reward.\n",
    "    - `done` is a boolean indicating whether the episode has ended.\n",
    "\n",
    "The output arguments are:\n",
    "\n",
    "- `policy`: The optimal policy derived from policy iteration. It is a NumPy array where each element represents the best action (as an integer index) to take in the corresponding state.\n",
    "- `V`: The optimal value function, a NumPy array of floats, where each element represents the value of being in the corresponding state under the optimal policy.\n",
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try On Your Own!\n",
    "\n",
    "You can try the given test environment to test your implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from minihouse.robotminihousemodel import MiniHouseV1\n",
    "from minihouse.minihousev1 import test_cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(VI_bool = True, index = 0, verbose=False):\n",
    "\n",
    "    instruction, goal_state, initial_conditions = test_cases[index]\n",
    "\n",
    "    env = MiniHouseV1(\n",
    "        instruction=instruction,\n",
    "        goal_state=goal_state,\n",
    "        initial_conditions=initial_conditions,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "    env.reset()\n",
    "    if verbose:\n",
    "        print(\"state: \", env.state_to_index(env.state))\n",
    "        print(\"num state: \", env.nS)\n",
    "        print(\"num actions: \", env.nA)\n",
    "        print()\n",
    "\n",
    "    ms = mdp_solver()\n",
    "    \n",
    "    if VI_bool:\n",
    "        policy, V = ms.value_iteration(0.9, 0.0001, env_nS=env.nS, env_nA=env.nA, env_transition=env.transition)\n",
    "        if verbose:\n",
    "            print(\"Value Iteration\")\n",
    "            print()\n",
    "    else:\n",
    "        policy, V = ms.policy_iteration(0.9, 0.0001, env_nS=env.nS, env_nA=env.nA, env_transition=env.transition)\n",
    "        if verbose:\n",
    "            print(\"Policy Iteration\")\n",
    "            print()\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"V: \", repr(V))\n",
    "        print()\n",
    "        print(\"policy: \", repr(policy))\n",
    "        print()\n",
    "    \n",
    "    env.reset()\n",
    "    \n",
    "    for i in range(100):\n",
    "        print()\n",
    "        print(f\"---------- Step: {i} ----------\")\n",
    "        \n",
    "        action = int(policy[env.state_to_index(env.state)])\n",
    "        obs, reward, done, _, _ = env.step(action)\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"obs: \", obs)\n",
    "            print(\"reward: \", reward)\n",
    "            print(\"done: \", done)\n",
    "        \n",
    "        if done:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test: Value Iteration\n",
    "Run the following code to test your implementation of value iteration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state:  9\n",
      "num state:  56\n",
      "num actions:  13\n",
      "\n",
      "MDP initialized!\n",
      "Value Iteration\n",
      "\n",
      "V:  array([499.99959766, 499.99963427, 499.99963427, 499.99966755,\n",
      "       499.99962964, 499.99966334, 499.99966334, 499.99969397,\n",
      "       306.62502756, 345.7146245 , 271.83106078, 240.86060754,\n",
      "       345.7146245 , 389.63008967, 306.62509833, 271.83112512,\n",
      "       438.96711366, 389.6300606 , 389.6300606 , 345.71466242,\n",
      "       438.96714564, 389.63008967, 389.63008967, 345.71468884,\n",
      "       345.71459253, 389.6300606 , 306.62506926, 271.8310987 ,\n",
      "       345.7146245 , 389.63008967, 306.62509833, 271.83112512,\n",
      "       345.71459253, 306.62506926, 389.6300606 , 345.71466242,\n",
      "       345.7146245 , 306.62509833, 389.63008967, 345.71468884,\n",
      "       271.83102881, 240.86057848, 306.62506926, 345.71466242,\n",
      "       271.83106078, 240.86060754, 306.62509833, 345.71468884,\n",
      "       494.39523867, 438.96718355, 438.96718355, 389.63012413,\n",
      "       494.39526773, 438.96720997, 438.96720997, 389.63014815])\n",
      "\n",
      "policy:  array([1., 4., 3., 0., 1., 1., 3., 0., 1., 4., 0., 2., 1., 6., 0., 2., 6.,\n",
      "       0., 0., 2., 6., 0., 0., 2., 1., 6., 0., 2., 1., 6., 0., 2., 2., 0.,\n",
      "       6., 2., 2., 0., 6., 2., 2., 0., 3., 6., 2., 0., 3., 6., 8., 0., 0.,\n",
      "       2., 8., 0., 0., 2.])\n",
      "\n",
      "\n",
      "---------- Step: 0 ----------\n",
      "action:  open the fridge\n",
      "obs:  You are at the kitchen.\n",
      "You see the kitchen is connected to the living room. The fridge is at the kitchen, the fridge is closed. \n",
      "reward:  -1\n",
      "done:  False\n",
      "\n",
      "---------- Step: 1 ----------\n",
      "action:  open the fridge\n",
      "obs:  You are at the kitchen.\n",
      "You see the kitchen is connected to the living room. The fridge is at the kitchen, the fridge is open. The apple is at the fridge. \n",
      "\n",
      "reward:  -1\n",
      "done:  False\n",
      "\n",
      "---------- Step: 2 ----------\n",
      "action:  pick the apple\n",
      "obs:  You are at the kitchen.\n",
      "You see the kitchen is connected to the living room. The fridge is at the kitchen, the fridge is open. The apple is picked by you. \n",
      "reward:  -1\n",
      "done:  False\n",
      "\n",
      "---------- Step: 3 ----------\n",
      "action:  move to the living room\n",
      "obs:  You are at the living room.\n",
      "You see the living room is connected to the kitchen, and the bedroom. The apple is picked by you. \n",
      "reward:  -1\n",
      "done:  False\n",
      "\n",
      "---------- Step: 4 ----------\n",
      "action:  place the apple at the table\n",
      "obs:  You are at the living room.\n",
      "You see the living room is connected to the kitchen, and the bedroom. \n",
      "reward:  50\n",
      "done:  True\n"
     ]
    }
   ],
   "source": [
    "test(index=0, VI_bool=True, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test: Policy Iteration\n",
    "\n",
    "Run the following code to test your implementation of policy iteration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state:  9\n",
      "num state:  56\n",
      "num actions:  13\n",
      "\n",
      "MDP initialized!\n",
      "Policy Iteration\n",
      "\n",
      "V:  array([499.9999996 , 499.9999996 , 499.9999996 , 499.9999996 ,\n",
      "       499.9999996 , 499.9999996 , 499.9999996 , 499.9999996 ,\n",
      "       306.6254346 , 345.7149945 , 271.83143079, 240.86094389,\n",
      "       345.7149945 , 389.63042598, 306.62543466, 271.83143084,\n",
      "       438.96751561, 389.63042598, 389.63042598, 345.71499455,\n",
      "       438.96751561, 389.63042598, 389.63042598, 345.71499455,\n",
      "       345.7149945 , 389.63042598, 306.62543466, 271.83143084,\n",
      "       345.7149945 , 389.63042598, 306.62543466, 271.83143084,\n",
      "       345.7149945 , 306.62543466, 389.63042598, 345.71499455,\n",
      "       345.7149945 , 306.62543466, 389.63042598, 345.71499455,\n",
      "       271.83143079, 240.86094389, 306.62543466, 345.71499455,\n",
      "       271.83143079, 240.86094389, 306.62543466, 345.71499455,\n",
      "       494.39560403, 438.96751567, 438.96751567, 389.63042603,\n",
      "       494.39560403, 438.96751567, 438.96751567, 389.63042603])\n",
      "\n",
      "policy:  array([1., 4., 3., 0., 1., 1., 3., 0., 1., 4., 0., 2., 1., 6., 0., 2., 6.,\n",
      "       0., 0., 2., 6., 0., 0., 2., 1., 6., 0., 2., 1., 6., 0., 2., 2., 0.,\n",
      "       6., 2., 2., 0., 6., 2., 2., 0., 3., 6., 2., 0., 3., 6., 8., 0., 0.,\n",
      "       2., 8., 0., 0., 2.])\n",
      "\n",
      "\n",
      "---------- Step: 0 ----------\n",
      "action:  open the fridge\n",
      "obs:  You are at the kitchen.\n",
      "You see the kitchen is connected to the living room. The fridge is at the kitchen, the fridge is open. The apple is at the fridge. \n",
      "\n",
      "reward:  -1\n",
      "done:  False\n",
      "\n",
      "---------- Step: 1 ----------\n",
      "action:  pick the apple\n",
      "obs:  You are at the kitchen.\n",
      "You see the kitchen is connected to the living room. The fridge is at the kitchen, the fridge is open. The apple is picked by you. \n",
      "reward:  -1\n",
      "done:  False\n",
      "\n",
      "---------- Step: 2 ----------\n",
      "action:  move to the living room\n",
      "obs:  You are at the living room.\n",
      "You see the living room is connected to the kitchen, and the bedroom. The apple is picked by you. \n",
      "reward:  -1\n",
      "done:  False\n",
      "\n",
      "---------- Step: 3 ----------\n",
      "action:  place the apple at the table\n",
      "obs:  You are at the living room.\n",
      "You see the living room is connected to the kitchen, and the bedroom. \n",
      "reward:  50\n",
      "done:  True\n"
     ]
    }
   ],
   "source": [
    "test(index=0, VI_bool=False, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎉 Congratulations!\n",
    "\n",
    "You have finished all the requirements for Problem 1.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2: Q-Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem, you are expected to implement the **Q-learning** algorithm to solve the searching problems in the `MiniHouse` environment. \n",
    "\n",
    "Similar to the previous problem, we provide a template for you to implement your own agents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mdp_solver_q_learning:\n",
    "    \n",
    "    def __init__(self):\n",
    "        print(\"MDP initialized for Q-learning!\")\n",
    "\n",
    "\n",
    "    def epsilon_greedy(self, Q, state, epsilon):\n",
    "        if np.random.rand() < epsilon:\n",
    "            \n",
    "            # ------- your code starts here ----- #\n",
    "            \n",
    "            \n",
    "            \n",
    "            # ------- your code ends here ------- #\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            # ------- your code starts here ----- #\n",
    "            \n",
    "            \n",
    "        \n",
    "            # ------- your code ends here ------- #\n",
    "   \n",
    "\n",
    "    def Q_learning(\n",
    "        self, alpha:float, gamma:float, theta:float, epsilon:float, env_nS:int, env_nA:int, env_transition, env, num_episodes=1000):\n",
    "        \"\"\"\n",
    "        Q-learning algorithm.\n",
    "        Args:\n",
    "            gamma: discount factor\n",
    "            theta: convergence threshold\n",
    "            env_nS: number of states\n",
    "            env_nA: number of actions\n",
    "            env_transition: transition function\n",
    "            num_episodes: number of episodes\n",
    "        Returns:\n",
    "            Q: learned Q-value function\n",
    "            rewards: rewards obtained in each episode\n",
    "        \"\"\"\n",
    "        Q = np.zeros((env_nS, env_nA))\n",
    "        rewards = []\n",
    "        \n",
    "        for episode in range(num_episodes):\n",
    "            env.reset()\n",
    "            state = env.state_to_index(env.state)\n",
    "            done = False\n",
    "            \n",
    "            while not done:\n",
    "                \n",
    "                # ------- your code starts here ----- #\n",
    "                \n",
    "                \n",
    "                \n",
    "                # ------- your code ends here ------- #\n",
    "        \n",
    "        return np.argmax(Q, axis=1), rewards\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🔍 Guideline\n",
    "\n",
    "Here, we introduce the role of each function in the above solution template. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epsilon-Greedy Action Selection\n",
    "The function is as follows:\n",
    "\n",
    "```python\n",
    "    def epsilon_greedy(self, Q, state, epsilon):\n",
    "        if np.random.rand() < epsilon:\n",
    "            \n",
    "            # ------- your code starts here ----- #\n",
    "\n",
    "            \n",
    "            # ------- your code ends here ------- #\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            # ------- your code starts here ----- #\n",
    " \n",
    "        \n",
    "            # ------- your code ends here ------- #\n",
    "```\n",
    "\n",
    "This function selects an action based on the epsilon-greedy strategy, which balances exploration and exploitation. With a probability of epsilon, it chooses a random action (exploration), and with a probability of 1 - epsilon, it chooses the action with the highest Q-value for the current state (exploitation).\n",
    "\n",
    "The input arguments are:\n",
    "\n",
    "- `Q`: The Q-value table, a 2D NumPy array where each element Q[state, action] represents the estimated value of taking an action in a state.\n",
    "- `state`: The current state index, an integer representing the current situation of the environment.\n",
    "- `epsilon`: The exploration rate, a float between 0 and 1 that determines the probability of choosing a random action.\n",
    "\n",
    "The output is:\n",
    "- An integer representing the index of the selected action.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning\n",
    "\n",
    "The function is as follows:\n",
    "\n",
    "```python\n",
    "def Q_learning(\n",
    "        self, alpha:float, gamma:float, theta:float, epsilon:float, env_nS:int, env_nA:int, env_transition, env, num_episodes=1000):\n",
    "        \"\"\"\n",
    "        Q-learning algorithm.\n",
    "        Args:\n",
    "            gamma: discount factor\n",
    "            theta: convergence threshold\n",
    "            env_nS: number of states\n",
    "            env_nA: number of actions\n",
    "            env_transition: transition function\n",
    "            num_episodes: number of episodes\n",
    "        Returns:\n",
    "            Q: learned Q-value function\n",
    "            rewards: rewards obtained in each episode\n",
    "        \"\"\"\n",
    "        Q = np.zeros((env_nS, env_nA))\n",
    "        rewards = []\n",
    "        \n",
    "        for episode in range(num_episodes):\n",
    "            env.reset()\n",
    "            state = env.state_to_index(env.state)\n",
    "            done = False\n",
    "            \n",
    "            while not done:\n",
    "                \n",
    "                # ------- your code starts here ----- #\n",
    "                \n",
    "     \n",
    "                \n",
    "                # ------- your code ends here ------- #\n",
    "        \n",
    "        return np.argmax(Q, axis=1), rewards\n",
    "```\n",
    "\n",
    "This function implements the Q-learning algorithm, a model-free reinforcement learning technique used to learn the quality of actions denoting how good it is to take an action from a particular state.\n",
    "\n",
    "The input arguments are:\n",
    "- `alpha`: The learning rate, a float that determines the extent to which the newly acquired information will override the old information.\n",
    "- `gamma`: The discount factor, a float that balances the importance of immediate and future rewards.\n",
    "- `theta`: The convergence threshold, not directly used in basic Q-learning but can be utilized for extensions or stopping criteria based on improvements.\n",
    "- `epsilon`: Exploration rate in the epsilon-greedy strategy, determining the trade-off between exploration and exploitation.\n",
    "- `env_nS`: The number of states in the environment, indicating how many distinct states the agent can be in.\n",
    "- `env_nA`: The number of actions available in the environment, indicating the range of actions the agent can take.\n",
    "- `env_transition`: The transition function of the environment that models the dynamics of the environment. It's used to simulate the next state and reward given a state-action pair.\n",
    "- `env`: The environment object itself, providing access to essential methods like reset and state_to_index.\n",
    "- `num_episodes`: The number of episodes to run the algorithm for, allowing the agent sufficient time to learn from its interactions with the environment.\n",
    "\n",
    "The outputs are:\n",
    "- `Q`: The learned Q-value function, represented as a 2D numpy array where each element [s, a] corresponds to the estimated value of taking action a in state s.\n",
    "- `rewards`: A list of accumulated rewards, one for each episode, indicating the total reward the agent has obtained during that episode.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try On Your Own!\n",
    "\n",
    "You can try the given test environment to test your implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from minihouse.robotminihousemodel import MiniHouseV1\n",
    "from minihouse.minihousev1 import test_cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test: Q-Learning\n",
    "\n",
    "Run the following code to test your implementation of the Q-learning algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_q_learning(Q_bool=False, index=0, num_episodes=1000, verbose=False, generate_solution=False):\n",
    "\n",
    "    instruction, goal_state, initial_conditions = test_cases[index]\n",
    "\n",
    "    env = MiniHouseV1(\n",
    "        instruction=instruction,\n",
    "        goal_state=goal_state,\n",
    "        initial_conditions=initial_conditions,\n",
    "        verbose=verbose\n",
    "    )\n",
    "    env.reset()\n",
    "    \n",
    "    \n",
    "    if verbose:\n",
    "        print(\"state: \", env.state_to_index(env.state))\n",
    "        print(\"num state: \", env.nS)\n",
    "        print(\"num actions: \", env.nA)\n",
    "        print()\n",
    "        \n",
    "    msq = mdp_solver_q_learning()\n",
    "\n",
    "\n",
    "    policy, V = msq.Q_learning(\n",
    "        alpha=0.1,\n",
    "        gamma=0.9,\n",
    "        theta=0.0001,\n",
    "        epsilon=0.1,\n",
    "        env_nS=env.nS,\n",
    "        env_nA=env.nA,\n",
    "        env_transition=env.transition,\n",
    "        env=env,\n",
    "        num_episodes=num_episodes,\n",
    "    )\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Policy Iteration\")\n",
    "        print()\n",
    "        print(\"V: \", repr(V))\n",
    "        print()\n",
    "        print(\"policy: \", repr(policy))\n",
    "        print()\n",
    "        \n",
    "    env.reset()\n",
    "    \n",
    "    for i in range(100):\n",
    "        print()\n",
    "        print(f\"---------- Step: {i} ----------\")\n",
    "        action = int(policy[env.state_to_index(env.state)])\n",
    "        # action = policy[env.state_to_index(env.state)].astype(int)\n",
    "        obs, reward, done, _, _ = env.step(action)\n",
    "        if verbose:\n",
    "            print(\"obs: \", obs)\n",
    "            print(\"reward: \", reward)\n",
    "            print(\"done: \", done)\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    if generate_solution:\n",
    "        np.savetxt(f'data/V_{index}.py', V, delimiter=',')\n",
    "\n",
    "        solution_values = np.loadtxt(f'data/V_{index}.py')\n",
    "\n",
    "        assert len(V) == len(solution_values), \\\n",
    "            'Length of Values is incorrect'\n",
    "\n",
    "        assert np.allclose(V, solution_values), \\\n",
    "            'Values incorrect'\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state:  9\n",
      "num state:  56\n",
      "num actions:  13\n",
      "\n",
      "MDP initialized for Q-learning!\n",
      "Policy Iteration\n",
      "\n",
      "V:  []\n",
      "\n",
      "policy:  array([0, 0, 0, 0, 0, 0, 0, 0, 1, 4, 0, 1, 1, 6, 1, 1, 6, 3, 1, 3, 6, 1,\n",
      "       1, 0, 3, 7, 1, 1, 2, 6, 1, 0, 0, 0, 0, 0, 3, 1, 7, 3, 3, 5, 1, 3,\n",
      "       3, 5, 5, 0, 8, 0, 0, 0, 8, 0, 0, 7])\n",
      "\n",
      "\n",
      "---------- Step: 0 ----------\n",
      "action:  open the fridge\n",
      "obs:  You are at the kitchen.\n",
      "You see the kitchen is connected to the living room. The fridge is at the kitchen, the fridge is closed. \n",
      "reward:  -1\n",
      "done:  False\n",
      "\n",
      "---------- Step: 1 ----------\n",
      "action:  open the fridge\n",
      "obs:  You are at the kitchen.\n",
      "You see the kitchen is connected to the living room. The fridge is at the kitchen, the fridge is open. The apple is at the fridge. \n",
      "\n",
      "reward:  -1\n",
      "done:  False\n",
      "\n",
      "---------- Step: 2 ----------\n",
      "action:  pick the apple\n",
      "obs:  You are at the kitchen.\n",
      "You see the kitchen is connected to the living room. The fridge is at the kitchen, the fridge is open. The apple is picked by you. \n",
      "reward:  -1\n",
      "done:  False\n",
      "\n",
      "---------- Step: 3 ----------\n",
      "action:  move to the living room\n",
      "obs:  You are at the living room.\n",
      "You see the living room is connected to the kitchen, and the bedroom. The apple is picked by you. \n",
      "reward:  -1\n",
      "done:  False\n",
      "\n",
      "---------- Step: 4 ----------\n",
      "action:  place the apple at the table\n",
      "obs:  You are at the living room.\n",
      "You see the living room is connected to the kitchen, and the bedroom. \n",
      "reward:  50\n",
      "done:  True\n"
     ]
    }
   ],
   "source": [
    "test_q_learning(index=0, num_episodes=1000, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎉 Congratulations!\n",
    "\n",
    "You have finished all the requirements for Problem 2.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3: Integrating GPT-4 for Decision Support in MiniHouse (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🔍 Guideline\n",
    "\n",
    "- **Prompt Design:** Crafting detailed and clear prompts that accurately describe the decision-making problem is crucial for obtaining useful suggestions from GPT-4.\n",
    "- **API Key:** Ensure you have a valid API key from OpenAI or the respective provider of the GPT-4 model you're using.\n",
    "- **Post-processing:** The action sequences generated by GPT-4 might require post-processing or validation to ensure they fit within the constraints and capabilities of the MiniHouse simulation.\n",
    "- **Integration:** This approach can serve as a high-level heuristic or guide for developing or refining decision-making strategies within your environment. Actual integration might involve translating natural language actions into environment-specific operations or commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try On Your Own!\n",
    "\n",
    "You can try the given test environment to test your implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
